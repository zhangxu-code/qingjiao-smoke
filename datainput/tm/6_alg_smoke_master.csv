title,datasource,result,code,timeout,expect
cnnface,"[{""dataSourceId"":184,""dataSourceMetadataId"":225,""dataServerId"":1,""varName"":""face_people""},{""dataSourceId"":184,""dataSourceMetadataId"":92,""dataServerId"":1,""varName"":""face_table""}]","[{""resultVarName"":""result"",""resultDest"":""-1""}]","import privpy as pp
import pnumpy as pnp

TABLE = pnp.reshape(pp.ss(""face_table""), (105, 512))
PEOPLE = pnp.reshape(pp.ss(""face_people""), (5, 512))

RES = []
for vec in PEOPLE:
    n_persons = TABLE.shape[0]
    dist = pnp.tile(vec, (n_persons, 1)) - TABLE
    dist = pnp.sum(dist * dist, axis=1)  # square distance
    index = pnp.argmin(dist)
    RES.append(index)
    
pp.reveal(pp.farr(RES), 'result')",,
matrix_factor,"[{""dataSourceId"":194,""dataSourceMetadataId"":105,""dataServerId"":1,""varName"":""movie1""},{""dataSourceId"":194,""dataSourceMetadataId"":122,""dataServerId"":1,""varName"":""movie2""}]","[{""resultVarName"":""result1"",""resultDest"":""-1""},{""resultVarName"":""result2"",""resultDest"":""-1""}]","import numpy as np
import privpy as pp
import pnumpy as pnp

def funk_svd(d_tmp, k_tmp, iter_times=60, alpha=0.01, learn_rate=0.001):
    '''
    funk_svd
    '''
    scalar = 0.001
    a, b = d_tmp.shape  # D size = m * n
    np.random.seed(2018)
    u = pp.farr(np.random.rand(a, k_tmp) * scalar)
    v = pp.farr(np.random.rand(k_tmp, b) * scalar)
    err_tmp = None
    for i in range(iter_times):
        print('round:', i)
        d_est = pnp.dot(u, v)
        err_tmp = d_tmp - d_est
        print('computed Err')
        u_grad = -2 * pnp.dot(err_tmp, v.trans()) + 2 * alpha * u
        v_grad = -2 * pnp.dot(u.trans(), err_tmp) + 2 * alpha * v
        u = u - learn_rate * u_grad
        v = v - learn_rate * v_grad
    print('updated u and v')
    return u, v

DATA1 = pnp.reshape(pp.ss(""movie1""), (50, 9724))
DATA2 = pnp.reshape(pp.ss(""movie2""), (50, 9724))
DATA = pnp.concatenate([DATA1, DATA2])

k = 10
ITER_NUM = 2

U_RES, V_RES = funk_svd(DATA, k, iter_times=ITER_NUM, learn_rate=0.0005, alpha=0.001)

pp.reveal(U_RES, ""result1"")
pp.reveal(V_RES, ""result2"")",,
logistic,"[{""dataSourceId"":269,""dataSourceMetadataId"":244,""dataServerId"":1,""varName"":""adult1""},{""dataSourceId"":175,""dataSourceMetadataId"":237,""dataServerId"":1,""varName"":""adult2""}]","[{""resultVarName"":""result"",""resultDest"":""-1""}]","import numpy as np
import privpy as pp
import pnumpy as pnp
import ptorch as pt 

class LogisticRegressionS(object):
    '''
    the logistic regression class, including msub functions.
    '''
    def __init__(self, q=0.01, num_iter=100000):
        self.q = q
        self.num_iter = num_iter
        self.theta = pnp.zeros([])

    def __add_intercept(self, x):
        '''
        add x shape ones matrix with x
        '''
        intercept = pnp.ones((x.shape[0], 1))
        return pnp.concatenate((intercept, x), axis=1)

    def __sigmoid(self, x):
        '''
        return 1 / (1 + np.exp(-x))
        '''
        tmp_result = pt.relu(x + 0.5)
        tmp_result = tmp_result - pt.relu(tmp_result - 1)
        return tmp_result

    def fit(self, x, y, batch=-1):
        '''
        fit
        '''
        x = self.__add_intercept(x)

        if batch < 0:
            batch = len(x)

        # weights initialization
        self.theta = pnp.zeros(x.shape[1])

        for i in range(self.num_iter):
            for j in range(0, len(x), batch):
                tmp_x = x[j:j+batch]
                tmp_y = y[j:j+batch]
                z = pnp.dot(tmp_x, self.theta)
                d = self.__sigmoid(z)
                gradient = pnp.dot(tmp_x.trans(), (d - tmp_y)) / len(tmp_y)
                self.theta -= self.q * gradient / np.sqrt(i + 1)

    def predict_prob(self, x):
        '''
        predice prob
        '''
        x = pp.farr(x)
        x = self.__add_intercept(x)
        return self.__sigmoid(pnp.dot(x, self.theta))

    def predict(self, x):
        '''
        predict
        '''
        return self.predict_prob(x)

NUM_ITER = 20
BATCH_SIZE = 100

DATA1 = pnp.reshape(pp.ss(""adult1""), (200, 124))
DATA2 = pnp.reshape(pp.ss(""adult2""), (200, 124))
X1, Y1 = DATA1[:, :-1], DATA1[:, -1]
X2, Y2 = DATA2[:, :-1], DATA2[:, -1]

X_S = pnp.concatenate([X1, X2])
Y_S = pnp.concatenate([Y1, Y2])

MODEL_S = LogisticRegressionS(q=0.1, num_iter=NUM_ITER)
MODEL_S.fit(X_S, Y_S, batch=BATCH_SIZE)

PRED_S = MODEL_S.predict(X_S)
pp.reveal(PRED_S, ""result"")",,
query_gold,"[{""dataSourceId"":172,""dataSourceMetadataId"":222,""dataServerId"":1,""varName"":""table""},{""dataSourceId"":172,""dataSourceMetadataId"":223,""dataServerId"":1,""varName"":""query""}]","[{""resultVarName"":""result"",""resultDest"":""-1""}]","import numpy as np
import privpy as pp
import pnumpy as pnp

def queryf(table_que, condition_que):
    '''
    query function
    '''
    commodity_code, date_low, date_up, time_low, time_up, price_low, price_up = condition_que
    date_que, time_que, price_que, code_que = table_que.trans()
    cdate_1 = date_low < date_que
    cdate_2 = (date_low == date_que) * (time_low * 1000 < time_que + 0.5)
    cdate_3 = date_que < date_up
    cdate_4 = (date_up == date_que) * (time_que - 0.5 < time_up * 1000)
    cdate_que = (1 - (1 - cdate_1) * (1 - cdate_2)) * (1 - (1 - cdate_3) * (1 - cdate_4))
    cprice_que = (1 - (price_que < price_low)) * (1 - (price_que > price_up))
    ccomid_que = (commodity_code == code_que)
    que = cdate_que * cprice_que * ccomid_que
    count_que = pnp.sum(que)
    return count_que

TABLE = pnp.reshape(pp.ss(""table""), (1940, 4))
QUERY = pnp.reshape(pp.ss(""query""), (2, 7))

res = []
for i in range(len(QUERY)):
    tmp_res = queryf(TABLE, QUERY[i])
    res.append(tmp_res)

pp.reveal(pp.farr(res), ""result"")",,
lstm_hsi,"[{""dataSourceId"":183,""dataSourceMetadataId"":90,""dataServerId"":1,""varName"":""data""},{""dataSourceId"":183,""dataSourceMetadataId"":226,""dataServerId"":1,""varName"":""model""}]","[{""resultVarName"":""result"",""resultDest"":""-1""}]","import numpy as np
import privpy as pp
import pnumpy as pnp

class LSTM(object):
    '''
    LSTM class
    '''

    def sigmoid(self, x, step_count=10):
        '''
        sigmoid function
        '''
        tmp_result = 0.5
        delta_x = x / step_count
        for i in range(step_count):
            derivate = tmp_result * (1 - tmp_result)
            tmp_result += delta_x * derivate
            i += 1
        return tmp_result

    def tanh(self, x):
        '''
        2sigmoid(2x) - 1 = tanh(x)
        '''
        return 2.0 * self.sigmoid(2.0 * x) - 1.0

    def __init__(self, input_size, hidden_size, w_ih, w_hh, b_ih, b_hh):
        self.input_size = input_size
        self.hidden_size = hidden_size

        # weights for current time series x_t. There are four gates:
        # ii for input gate (update the choice of sigmoid)
        # if for forgive gate
        # ig for input gate (update to the value of tanh)
        # io for output gate
        self.w_ii = w_ih[:hidden_size]
        self.w_if = w_ih[hidden_size:2 * hidden_size]
        self.w_ig = w_ih[2 * hidden_size:3 * hidden_size]
        self.w_io = w_ih[3 * hidden_size:4 * hidden_size]

        self.b_ii = b_ih[:hidden_size]
        self.b_if = b_ih[hidden_size:2 * hidden_size]
        self.b_ig = b_ih[2 * hidden_size:3 * hidden_size]
        self.b_io = b_ih[3 * hidden_size:4 * hidden_size]

        # weights for previous hidden h_(t_1), concatenent to x_t
        self.w_hi = w_hh[:hidden_size]
        self.w_hf = w_hh[hidden_size:2 * hidden_size]
        self.w_hg = w_hh[2 * hidden_size:3 * hidden_size]
        self.w_ho = w_hh[3 * hidden_size:4 * hidden_size]

        self.b_hi = b_hh[:hidden_size]
        self.b_hf = b_hh[hidden_size:2 * hidden_size]
        self.b_hg = b_hh[2 * hidden_size:3 * hidden_size]
        self.b_ho = b_hh[3 * hidden_size:4 * hidden_size]

    def pass_cell(self, x_c, h_p, c_p):  # c - current, p - previous
        '''
        pass cell
        '''
        i_t = self.sigmoid(
            pnp.dot(self.w_ii, x_c) + self.b_ii + pnp.dot(self.w_hi, h_p) +
            self.b_hi)

        f_t = self.sigmoid(
            pnp.dot(self.w_if, x_c) + self.b_if + pnp.dot(self.w_hf, h_p) +
            self.b_hf)

        g_t = self.tanh(
            pnp.dot(self.w_ig, x_c) + self.b_ig + pnp.dot(self.w_hg, h_p) +
            self.b_hg)

        o_t = self.sigmoid(
            pnp.dot(self.w_io, x_c) + self.b_io + pnp.dot(self.w_ho, h_p) +
            self.b_ho)

        c_c = f_t * c_p + i_t * g_t
        h_c = o_t * self.tanh(c_c)

        return h_c, c_c

    def predict(self, input_t, h_0, c_0):
        '''
        predict
        '''
        (seq_len, input_size) = input_t.shape
        if input_size != self.input_size:
            raise Exception('The input_size of the input is incorrect!')

        h_p, c_p = h_0, c_0
        output = []

        for k in range(seq_len):
            x_c = input_t[k]
            h_p, c_p = self.pass_cell(x_c, h_p, c_p)
            output.append(h_p)

        return pp.farr(output)

# The model hyper-parameters

globals()['input_size'] = 24
globals()['hidden_size'] = 24
globals()['seq_len'] = 10

MODEL = pp.ss(""model"")

globals()['weight_emb'] = pnp.reshape(MODEL[:24], (24,))
globals()['bias_emb'] = pnp.reshape(MODEL[24:48], (24,))
globals()['weight_out'] = pnp.reshape(MODEL[48:72], (24,))
globals()['bias_out'] = MODEL[72]
globals()['weight_ih'] = pnp.reshape(MODEL[73:2377], (96, 24))
globals()['bias_ih'] = pnp.reshape(MODEL[2377:2473], (96,))
globals()['weight_hh'] = pnp.reshape(MODEL[2473:4777], (96, 24))
globals()['bias_hh'] = pnp.reshape(MODEL[4777:4873], (96,))

X_TEST = pnp.reshape(pp.ss(""data""), (3, 10))

globals()['lstm'] = LSTM(input_size, hidden_size, weight_ih, weight_hh, bias_ih, bias_hh)

def compute(x, g_tmp, c_tmp):  # X is of shape (1, 10)
    '''
    compute
    '''
    x_mat = pnp.tile(pnp.reshape(x, (seq_len, 1)), (
        1,
        input_size,
    ))  # 10 x 24
    emb_weight_mat = pnp.tile(weight_emb, (seq_len, 1))  # 10 x 24
    emb_bias_mat = pnp.tile(bias_emb, (seq_len, 1))  # 10 x 24
    pass_emb_layer = x_mat * emb_weight_mat + emb_bias_mat  # of shape (10, 24)

    # predictions = []
    y_lstm_pred = lstm.predict(pass_emb_layer, g_tmp, c_tmp)[-1]

    y_out = pnp.dot(weight_out, y_lstm_pred) + bias_out

    return y_out

def restore(array):
    '''
    restore array
    '''
    mean_arr = 24706.462699077954
    std_arr = 3113.3302180457677
    return array * std_arr + mean_arr

g = pnp.zeros(hidden_size)
c = pnp.zeros(hidden_size)
Y_PREDS = []
X_LEN = len(X_TEST)
for j in range(X_LEN):
    y_pred = restore(compute(X_TEST[j], g, c))
    Y_PREDS.append(y_pred)
Y_PREDS = pp.farr(Y_PREDS)

pp.reveal(Y_PREDS, ""result"")",,
nn_mnist,"[{""dataSourceId"":254,""dataSourceMetadataId"":218,""dataServerId"":1,""varName"":""mnist_weight""},{""dataSourceId"":254,""dataSourceMetadataId"":219,""dataServerId"":1,""varName"":""mnist_image""}]","[{""resultVarName"":""result"",""resultDest"":""-1""}]","import numpy as np
import privpy as pp
import pnumpy as pnp
import ptorch as pt

def parse_weight(weight_par):
    '''
    parse weight
    '''
    result = []
    shape = [784, 625, 625, 10]
    start = 0
    for i in range(1, len(shape)):
        step = shape[i-1] * shape[i]
        result.append(pnp.reshape(weight_par[start:start+step], (shape[i], shape[i-1])))
        start += step
    return result

def inference(img_inf, w_inf):
    '''
    inference
    '''
    tmp_x = img_inf
    w_len = len(w_inf)
    for i in range(w_len):
        tmp_x = pnp.dot(tmp_x, w_inf[i].trans())
        if i < len(w_inf) - 1:
            tmp_x = pt.relu(tmp_x)
    return pnp.argmax(tmp_x, axis=1)

WEIGHT = pp.ss(""mnist_weight"") # shape = 784 * 625 + 625 * 625 + 625 * 10
IMG = pnp.reshape(pp.ss(""mnist_image""), (10, 784))
w = parse_weight(WEIGHT)

PREDS = inference(IMG, w)

pp.reveal(PREDS, ""result"")",,